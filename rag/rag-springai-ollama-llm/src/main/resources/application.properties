spring.application.name=rag-springai-ollama-llm

spring.threads.virtual.enabled=true
spring.mvc.problemdetails.enabled=true

spring.ai.ollama.chat.options.model=mistral
spring.ai.ollama.chat.options.temperature=0.3
spring.ai.ollama.chat.options.top-k=2
spring.ai.ollama.chat.options.top-p=0.2

spring.ai.ollama.embedding.options.model=nomic-embed-text

spring.ai.vectorstore.redis.index=vector_store
spring.ai.vectorstore.redis.prefix=ai
# only for demo purpose
spring.ai.vectorstore.redis.initialize-schema=true

spring.ai.ollama.baseUrl=http://localhost:11434

spring.testcontainers.beans.startup=parallel

##Observability
spring.ai.chat.observations.include-completion=true
spring.ai.chat.observations.include-prompt=true

management.endpoints.web.exposure.include=*
management.metrics.tags.service.name=${spring.application.name}
management.tracing.sampling.probability=1.0
management.otlp.tracing.endpoint=http://localhost:4318/v1/traces
