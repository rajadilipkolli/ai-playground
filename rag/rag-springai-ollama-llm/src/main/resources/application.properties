spring.application.name=rag-springai-ollama-llm

spring.threads.virtual.enabled=true
spring.mvc.problemdetails.enabled=true

spring.ai.ollama.init.pull-model-strategy=WHEN_MISSING
spring.ai.ollama.chat.options.model=mistral
spring.ai.ollama.chat.options.temperature=0.3
spring.ai.ollama.chat.options.top-k=2
spring.ai.ollama.chat.options.top-p=0.2

spring.ai.ollama.embedding.options.model=nomic-embed-text

#PgVector
spring.ai.vectorstore.observations.include-query-response=true
spring.ai.vectorstore.pgvector.initialize-schema=true

spring.http.client.connect-timeout=PT1M
spring.http.client.read-timeout=PT5M

spring.testcontainers.beans.startup=parallel

##Observability
spring.ai.chat.observations.include-completion=true
spring.ai.chat.observations.include-prompt=true
spring.ai.chat.client.observations.include-input=true

management.endpoints.web.exposure.include=*
management.metrics.tags.service.name=${spring.application.name}
management.tracing.sampling.probability=1.0
management.otlp.tracing.endpoint=http://localhost:4318/v1/traces
management.otlp.logging.endpoint=http://localhost:4318/v1/logs

logging.level.org.springframework.ai.rag=info
